{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d441f53",
   "metadata": {},
   "source": [
    "# Filter tokens related to numbers or digits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec62bb9",
   "metadata": {},
   "source": [
    "In this notebook, we filter tokens related to digits or numbers. It should be executed for both vicuna and llama2 tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9a36e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T21:34:47.161320Z",
     "start_time": "2024-02-19T21:34:34.134658Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c647dbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:13:02.971991Z",
     "start_time": "2023-11-09T18:13:02.948515Z"
    }
   },
   "outputs": [],
   "source": [
    "#os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/mnt/hdd-nfs/mgubri/models_hf/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24966e66",
   "metadata": {},
   "source": [
    "### Load tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2f64c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T21:34:49.843474Z",
     "start_time": "2024-02-19T21:34:47.160334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dcb19f043de2497f88b792551d0402eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# llama2\n",
    "model_name = 'llama2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04692968",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T07:32:11.352072Z",
     "start_time": "2023-12-22T07:32:10.490536Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# vicuna\n",
    "model_name = 'vicuna'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.3\", use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf1b77",
   "metadata": {},
   "source": [
    "## Detect token with multiple digits\n",
    "\n",
    "Check that there are none tokens that contains multiple digits (only 10 single digit tokens). Because they are not filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29129385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:28:41.642105Z",
     "start_time": "2023-11-09T18:28:40.996985Z"
    }
   },
   "outputs": [],
   "source": [
    "for d in list(string.digits):\n",
    "    for k,v in tokenizer.get_vocab().items():\n",
    "        if d in k and k != d and not k.startswith('<0x'):\n",
    "            print(f'WARNING! the following token is not filtered: {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70835fa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:28:41.760111Z",
     "start_time": "2023-11-09T18:28:41.698424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁De\n",
      "▁Le\n",
      "▁Me\n",
      "De\n",
      "Le\n",
      "Me\n",
      "▁Ce\n",
      "▁Ve\n",
      "▁XIXe\n"
     ]
    }
   ],
   "source": [
    "# list century tokens (eg. XIXe)\n",
    "for k,v in tokenizer.get_vocab().items():\n",
    "    if re.search(r'[IVXLCDM]+e$',k): # centeray: XIVe\n",
    "       print(k)\n",
    "# XIXe added to the list of forbidden words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb3626",
   "metadata": {},
   "source": [
    "## List roman numerals tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b2bab02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:28:41.901094Z",
     "start_time": "2023-11-09T18:28:41.802727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roman numerals tokens: ▁I\n",
      "Roman numerals tokens: ▁C\n",
      "Roman numerals tokens: ▁M\n",
      "Roman numerals tokens: ▁D\n",
      "Roman numerals tokens: ▁L\n",
      "Roman numerals tokens: ▁V\n",
      "Roman numerals tokens: ▁X\n",
      "Roman numerals tokens: ▁II\n",
      "Roman numerals tokens: ML\n",
      "Roman numerals tokens: II\n",
      "Roman numerals tokens: CC\n",
      "Roman numerals tokens: DI\n",
      "Roman numerals tokens: ▁III\n",
      "Roman numerals tokens: LI\n",
      "Roman numerals tokens: III\n",
      "Roman numerals tokens: ▁VI\n",
      "Roman numerals tokens: ▁XV\n",
      "Roman numerals tokens: IV\n",
      "Roman numerals tokens: MD\n",
      "Roman numerals tokens: CL\n",
      "Roman numerals tokens: ▁XX\n",
      "Roman numerals tokens: XX\n",
      "Roman numerals tokens: IX\n",
      "Roman numerals tokens: CD\n",
      "Roman numerals tokens: ▁IV\n",
      "Roman numerals tokens: ▁CD\n",
      "Roman numerals tokens: MM\n",
      "Roman numerals tokens: CI\n",
      "Roman numerals tokens: MI\n",
      "Roman numerals tokens: ▁XIX\n",
      "Roman numerals tokens: MC\n",
      "Roman numerals tokens: DC\n",
      "Roman numerals tokens: ▁VII\n",
      "Roman numerals tokens: ▁DC\n",
      "Roman numerals tokens: ▁XVIII\n",
      "Roman numerals tokens: ▁XVI\n",
      "Roman numerals tokens: CV\n",
      "Roman numerals tokens: ▁VIII\n",
      "Roman numerals tokens: ▁XIII\n",
      "Roman numerals tokens: ▁IX\n",
      "Roman numerals tokens: ▁XVII\n",
      "Roman numerals tokens: ▁XIV\n",
      "Roman numerals tokens: ▁XII\n",
      "Roman numerals tokens: ▁CL\n",
      "Roman numerals tokens: ▁LI\n",
      "Roman numerals tokens: VI\n",
      "Roman numerals tokens: ▁XI\n",
      "Roman numerals tokens: ▁CC\n",
      "Roman numerals tokens: DL\n",
      "Roman numerals tokens: ▁MD\n",
      "Roman numerals tokens: ▁MC\n",
      "Roman numerals tokens: ▁DI\n",
      "Roman numerals tokens: ▁XXX\n",
      "Roman numerals tokens: XXX\n",
      "Roman numerals tokens: ▁ML\n",
      "Roman numerals tokens: ▁CLI\n",
      "Roman numerals tokens: CM\n",
      "Roman numerals tokens: ▁CV\n",
      "Roman numerals tokens: ▁CI\n",
      "Roman numerals tokens: CLI\n",
      "Roman numerals tokens: XV\n",
      "Roman numerals tokens: ▁MM\n",
      "Roman numerals tokens: I\n",
      "Roman numerals tokens: C\n",
      "Roman numerals tokens: M\n",
      "Roman numerals tokens: D\n",
      "Roman numerals tokens: L\n",
      "Roman numerals tokens: V\n",
      "Roman numerals tokens: X\n",
      "Filtered tokens: 69\n"
     ]
    }
   ],
   "source": [
    "#roman_numerals_voc = ['I', 'V', 'X', 'L', 'C', 'D', 'M'] \n",
    "#spaces = ['▁', ' ']\n",
    "def is_roman_numerals(string):\n",
    "    \"\"\"\n",
    "    Source: https://stackoverflow.com/questions/267399/how-do-you-match-only-valid-roman-numerals-with-a-regular-expression\n",
    "    Modified to go to 9999\n",
    "    \"\"\"\n",
    "    string = string.strip('▁').strip(' ')\n",
    "    if len(string) == 0:\n",
    "        return False\n",
    "    return bool(re.search(r'^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$',string))\n",
    "\n",
    "roman_numerals_tokens = {}\n",
    "for k,v in tokenizer.get_vocab().items():\n",
    "    if is_roman_numerals(k):\n",
    "        roman_numerals_tokens[k] = v\n",
    "        print(f'Roman numerals tokens: {k}')\n",
    "        \n",
    "print(f'Filtered tokens: {len(roman_numerals_tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e95c6b",
   "metadata": {},
   "source": [
    "## Export tokens that relates to a digits\n",
    "\n",
    "Filter token that tokens correspond to words that relate to a digit or a number. We take case, separation token, and plural into account.\n",
    "\n",
    "The CSV contains:\n",
    "- digits: 0,1,2,...\n",
    "- words number: one,two,Hundred,Thousand,etc.\n",
    "- months: january, etc.\n",
    "- day of the week\n",
    "- n-th: First, Second, Third, Fourth, etc.\n",
    "- cardinal prefixes: Uni, Bi, Tri, oct, dec, etc.\n",
    "- geometry: Octagon,triangle, etc.\n",
    "- others: Null,Void,Single,Unity,Decimal, etc.\n",
    "- romans numerals: D, XIV, etc.\n",
    "- century name: XIXe\n",
    "- repeated `X`: xx, XXX, etc.\n",
    "- latin-based numbers: milli, centi, dec, quadr, etc. \n",
    "- abbreviations of months and days of weeks: Jun, Aug, Sun, Mon\n",
    "\n",
    "Translate using Google Translate words number, months and days of the week into: FR, ES, IT, DE, PT\n",
    "with manual corrections. For example `May` -> `Mayo` instead of `Puede` (can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7716ff22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:28:42.024545Z",
     "start_time": "2023-11-09T18:28:42.005972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445 words to filter\n"
     ]
    }
   ],
   "source": [
    "df_words = pd.read_csv('../data/filter_tokens/filter_words_number.csv', header=None)\n",
    "list_words = df_words[0].to_list()\n",
    "print(f'{len(list_words)} words to filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad5500a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T07:35:10.368381Z",
     "start_time": "2023-12-22T07:35:10.286120Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_token(s, tokenizer, model_name, error_on_multiple_tokens=True):\n",
    "    list_tokens = tokenizer.encode(s, add_special_tokens=False)\n",
    "    if model_name in ['llama2', 'vicuna']:\n",
    "        # remove the SPIECE_UNDERLINE token that is added by sentencepiece \n",
    "        if list_tokens[0] == 29871:\n",
    "            list_tokens.pop(0)\n",
    "    else:\n",
    "        raise NotImplementedError('model_name not implemented')\n",
    "    if len(list_tokens) != 1:\n",
    "        if error_on_multiple_tokens:\n",
    "            raise ValueError(f'Does not correspond to a single token: {list_tokens}')\n",
    "        else:\n",
    "            return None\n",
    "    return list_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff6e2d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:29:22.122933Z",
     "start_time": "2023-11-09T18:28:42.083255Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_vocab = {}\n",
    "for word in list_words:\n",
    "    #print(word)\n",
    "    for k,v in tokenizer.get_vocab().items():\n",
    "        if word.lower() == k.lower().strip('▁').rstrip('s'):\n",
    "            # ignore case, remove space token, remove plural (s)\n",
    "            filtered_vocab[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f28f962",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:29:22.141941Z",
     "start_time": "2023-11-09T18:29:22.129252Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_vocab = {**filtered_vocab, **roman_numerals_tokens, **century_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7922989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:29:22.176738Z",
     "start_time": "2023-11-09T18:29:22.151176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'0': 29900,\n '1': 29896,\n '2': 29906,\n '3': 29941,\n '4': 29946,\n '5': 29945,\n '6': 29953,\n '7': 29955,\n '8': 29947,\n '9': 29929,\n '▁zero': 5225,\n 'zero': 9171,\n 'Zero': 24214,\n '▁zeros': 24786,\n '▁Zero': 28933,\n 'one': 650,\n '▁one': 697,\n 'ones': 2873,\n '▁One': 3118,\n 'One': 6716,\n '▁ones': 6743,\n 'ONE': 12413,\n '▁two': 1023,\n '▁Two': 7803,\n 'two': 10184,\n 'Two': 13985,\n '▁three': 2211,\n '▁Three': 12753,\n 'three': 17536,\n 'Three': 28575,\n '▁four': 3023,\n '▁Four': 12458,\n 'four': 17823,\n '▁five': 5320,\n 'five': 20818,\n '▁Five': 22853,\n '▁six': 4832,\n '▁Six': 18372,\n 'six': 28319,\n '▁seven': 9881,\n '▁Seven': 26647,\n '▁eight': 9475,\n '▁nine': 14183,\n 'ten': 841,\n '▁ten': 3006,\n '▁Ten': 12444,\n '▁tens': 25187,\n '▁eleven': 28121,\n '▁twelve': 17680,\n '▁fifteen': 25020,\n '▁twenty': 10081,\n '▁thirty': 17058,\n '▁forty': 20949,\n '▁fifty': 19044,\n '▁hundred': 6893,\n '▁hundreds': 21006,\n '▁thousand': 10405,\n '▁thousands': 17202,\n '▁million': 7284,\n '▁millions': 14746,\n '▁billion': 24464,\n '▁January': 5490,\n '▁February': 6339,\n '▁March': 4779,\n '▁march': 8575,\n '▁April': 3786,\n '▁april': 17187,\n '▁may': 1122,\n '▁May': 2610,\n 'May': 12703,\n 'may': 13029,\n '▁June': 5306,\n '▁July': 5468,\n '▁August': 3111,\n '▁august': 15251,\n 'August': 26197,\n '▁September': 3839,\n '▁september': 18251,\n '▁October': 5533,\n '▁November': 3979,\n '▁november': 14530,\n '▁December': 5846,\n '▁december': 13034,\n '▁Monday': 27822,\n '▁Friday': 28728,\n '▁Saturday': 24211,\n '▁Sunday': 16340,\n '▁null': 1870,\n '▁NULL': 4265,\n 'null': 4304,\n 'Null': 7327,\n 'NULL': 10074,\n '▁Null': 19014,\n '▁void': 1780,\n 'void': 5405,\n '▁Void': 29434,\n '▁single': 2323,\n 'single': 14369,\n 'Single': 15771,\n '▁Single': 16740,\n '▁Singles': 22065,\n '▁singles': 22102,\n 'unity': 6997,\n '▁unity': 20107,\n '▁Unity': 20872,\n '▁solo': 6651,\n '▁Solo': 29687,\n '▁primary': 7601,\n 'primary': 16072,\n 'Primary': 26666,\n '▁Primary': 28267,\n '▁PRIMARY': 29778,\n '▁double': 3765,\n 'double': 8896,\n '▁Double': 11599,\n 'Double': 11843,\n '▁doubles': 27641,\n '▁pair': 5101,\n '▁pairs': 11000,\n 'pair': 18784,\n 'Pair': 20547,\n '▁triple': 21954,\n '▁square': 6862,\n 'square': 17619,\n '▁Square': 19256,\n '▁squares': 25256,\n 'uni': 3909,\n 'Uni': 8110,\n 'Unis': 11604,\n '▁bis': 2652,\n '▁Bi': 3457,\n '▁bi': 4768,\n 'bi': 5365,\n 'BI': 12809,\n '▁Bis': 16818,\n 'bis': 18809,\n 'Bi': 20517,\n '▁tri': 3367,\n 'tri': 3626,\n '▁Tri': 8602,\n 'Tri': 29565,\n '▁Quint': 29223,\n '▁sex': 7916,\n 'sex': 14167,\n '▁Sex': 21703,\n '▁sept': 4843,\n '▁Sept': 28742,\n '▁oct': 4725,\n '▁Oct': 4756,\n 'oct': 20082,\n 'Oct': 25375,\n '▁dec': 1602,\n '▁Dec': 3826,\n 'Dec': 6185,\n 'dec': 7099,\n '▁first': 937,\n '▁First': 3824,\n 'first': 4102,\n 'First': 6730,\n '▁second': 1473,\n '▁Second': 6440,\n '▁seconds': 6923,\n 'second': 7496,\n 'Second': 11863,\n 'seconds': 23128,\n 'Seconds': 27535,\n '▁third': 4654,\n '▁Third': 18008,\n 'third': 22585,\n '▁fourth': 11582,\n '▁fifth': 18615,\n '▁sixth': 25963,\n '▁none': 5642,\n '▁None': 6213,\n 'None': 8516,\n 'none': 9290,\n '▁Millionen': 26158,\n '▁singleton': 27130,\n '▁unique': 5412,\n 'unique': 13092,\n '▁binary': 7581,\n 'binary': 19541,\n 'Binary': 25196,\n '▁Binary': 29479,\n '▁couple': 7303,\n '▁twice': 8951,\n '▁dozen': 24231,\n '▁triangle': 17205,\n 'triangle': 26701,\n '▁septiembre': 9199,\n '▁septembre': 9355,\n '▁secondo': 15015,\n '▁secondary': 16723,\n '▁seconda': 18740,\n '▁seconde': 26617,\n 'un': 348,\n '▁un': 443,\n '▁Un': 853,\n 'Un': 2525,\n 'UN': 3904,\n 'uns': 6948,\n '▁UN': 8291,\n '▁uns': 9644,\n 'Uns': 25807,\n '▁deux': 4239,\n '▁Deux': 26079,\n '▁quatre': 12134,\n '▁cinq': 17256,\n '▁huit': 27052,\n '▁dix': 23386,\n '▁cent': 1644,\n 'cent': 1760,\n '▁Cent': 2895,\n 'Cent': 23369,\n '▁janvier': 8891,\n '▁février': 10295,\n '▁avril': 9417,\n '▁mais': 3503,\n '▁mai': 5530,\n '▁Mai': 6868,\n '▁Mais': 11948,\n 'mai': 24402,\n '▁juin': 8781,\n '▁juillet': 9148,\n '▁août': 10158,\n '▁octobre': 9419,\n '▁novembre': 7005,\n '▁décembre': 9367,\n '▁uno': 6888,\n 'uno': 9447,\n 'unos': 12609,\n '▁unos': 22660,\n '▁cuatro': 19545,\n '▁cinco': 21357,\n '▁once': 2748,\n '▁Once': 9038,\n 'once': 10646,\n 'onces': 17330,\n 'Once': 26222,\n '▁mil': 2316,\n '▁Mil': 3833,\n 'mil': 23853,\n 'Mil': 29316,\n '▁enero': 8529,\n '▁febrero': 9091,\n '▁marzo': 6612,\n '▁abril': 8047,\n '▁mayo': 7502,\n '▁junio': 9019,\n '▁julio': 8996,\n '▁agosto': 6754,\n '▁octubre': 8644,\n '▁noviembre': 9350,\n '▁diciembre': 9060,\n '▁due': 2861,\n '▁Due': 16809,\n '▁dues': 27447,\n 'due': 29123,\n 'tre': 2484,\n '▁tre': 2578,\n 'tres': 5888,\n '▁Tre': 6479,\n '▁tres': 9941,\n '▁quattro': 21842,\n '▁sei': 13106,\n '▁seis': 26251,\n 'otto': 9693,\n '▁Otto': 13832,\n '▁otto': 15999,\n '▁gennaio': 16111,\n '▁febbraio': 18486,\n '▁aprile': 18998,\n '▁maggio': 16536,\n '▁giugno': 16935,\n '▁luglio': 17154,\n '▁settembre': 16621,\n '▁ottobre': 18395,\n '▁dicembre': 17309,\n '▁zwei': 7325,\n '▁drei': 9697,\n 'vier': 7214,\n '▁vier': 8545,\n '▁Vier': 23650,\n '▁fünf': 17054,\n '▁sieben': 29447,\n 'acht': 5860,\n '▁acht': 22019,\n 'elf': 761,\n '▁Januar': 7116,\n '▁Februar': 8196,\n '▁Juni': 7452,\n '▁juni': 17340,\n '▁Juli': 7603,\n '▁juli': 14396,\n '▁Oktober': 7619,\n '▁oktober': 19306,\n '▁Dezember': 7860,\n 'um': 398,\n '▁um': 1922,\n 'UM': 5005,\n '▁Um': 6379,\n 'ums': 6762,\n '▁Dez': 7383,\n '▁dez': 18466,\n '▁Janeiro': 20883,\n 'xx': 4419,\n '▁XX': 6193,\n 'XX': 6247,\n '▁xx': 15473,\n 'xxx': 12353,\n '▁XXX': 22615,\n 'XXX': 22791,\n 'xxxx': 14633,\n 'XXXX': 19165,\n '▁decimal': 13677,\n 'Decimal': 23307,\n '▁quadr': 15448,\n '▁Jan': 2627,\n '▁jan': 5496,\n 'jan': 8931,\n 'Jan': 26626,\n '▁feb': 6659,\n '▁Feb': 26319,\n '▁Mar': 1085,\n '▁mar': 1766,\n 'mar': 3034,\n 'Mar': 7083,\n '▁mars': 7438,\n '▁Mars': 16852,\n '▁MAR': 23851,\n '▁apr': 21783,\n '▁jun': 4707,\n '▁Jun': 8378,\n '▁Jul': 2739,\n '▁jul': 5757,\n 'Jul': 27501,\n 'aug': 2987,\n '▁aug': 11307,\n '▁Aug': 22333,\n '▁sep': 16345,\n 'sep': 19570,\n '▁Sep': 29639,\n '▁nov': 2420,\n '▁Nov': 2864,\n 'nov': 13715,\n 'Nov': 25363,\n '▁mon': 1601,\n '▁Mon': 2598,\n 'mon': 3712,\n 'Mon': 7185,\n 'MON': 22877,\n 'wed': 8734,\n '▁wed': 14837,\n '▁Wed': 15050,\n '▁thus': 4550,\n '▁Thus': 6549,\n '▁fri': 3484,\n 'fri': 7932,\n '▁Fri': 11169,\n 'Fri': 27034,\n '▁sat': 3290,\n '▁Sat': 12178,\n '▁sun': 6575,\n '▁Sun': 8991,\n 'sun': 11445,\n '▁I': 306,\n '▁C': 315,\n '▁M': 341,\n '▁D': 360,\n '▁L': 365,\n '▁V': 478,\n '▁X': 1060,\n '▁II': 1944,\n 'ML': 1988,\n 'II': 2687,\n 'CC': 4174,\n 'DI': 4571,\n '▁III': 4786,\n 'LI': 5265,\n 'III': 5287,\n '▁VI': 5473,\n '▁XV': 5488,\n 'IV': 5667,\n 'MD': 5773,\n 'CL': 6154,\n 'IX': 6415,\n 'CD': 6530,\n '▁IV': 6599,\n '▁CD': 7307,\n 'MM': 7428,\n 'CI': 8426,\n 'MI': 10403,\n '▁XIX': 10634,\n 'MC': 12513,\n 'DC': 12696,\n '▁VII': 13408,\n '▁DC': 13681,\n '▁XVIII': 14271,\n '▁XVI': 14488,\n 'CV': 15633,\n '▁VIII': 15682,\n '▁XIII': 16714,\n '▁IX': 16841,\n '▁XVII': 17031,\n '▁XIV': 17071,\n '▁XII': 17172,\n '▁CL': 17332,\n '▁LI': 17705,\n 'VI': 18118,\n '▁XI': 18488,\n '▁CC': 19178,\n 'DL': 19558,\n '▁MD': 20672,\n '▁MC': 21271,\n '▁DI': 22471,\n '▁ML': 23158,\n '▁CLI': 24492,\n 'CM': 24494,\n '▁CV': 25778,\n '▁CI': 25781,\n 'CLI': 27205,\n 'XV': 28462,\n '▁MM': 28880,\n 'I': 29902,\n 'C': 29907,\n 'M': 29924,\n 'D': 29928,\n 'L': 29931,\n 'V': 29963,\n 'X': 29990,\n '▁Ve': 8980,\n '▁XIXe': 28250}"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09006953",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:29:22.178966Z",
     "start_time": "2023-11-09T18:29:22.163556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "432"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d662ca1",
   "metadata": {},
   "source": [
    "### Check that we do not miss tokens \n",
    "\n",
    "Print tokens that contain a forbidden word, that are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "398538f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:29:59.894812Z",
     "start_time": "2023-11-09T18:29:22.203726Z"
    }
   },
   "outputs": [],
   "source": [
    "# ignored words\n",
    "ignored_tokens = []  # key: token ignored, value: source word\n",
    "for word in list_words:\n",
    "    for k,v in tokenizer.get_vocab().items():\n",
    "        if word.lower() in k.lower() and k not in filtered_vocab.keys() and not k.startswith('<0x'):\n",
    "            ignored_tokens.append({'token_ignored': k, 'word': word})\n",
    "df_ignored = pd.DataFrame(ignored_tokens)\n",
    "df_ignored.to_csv(f'../data/filter_tokens/ignored_tokens_{model_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03a77cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:30:01.817515Z",
     "start_time": "2023-11-09T18:29:59.895192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁Univers\n",
      "unic\n",
      "▁University\n",
      "▁United\n",
      "unicip\n",
      "▁univers\n",
      "▁unit\n",
      "unit\n",
      "▁municip\n",
      "▁communic\n",
      "▁Union\n",
      "▁community\n",
      "Unit\n",
      "▁uniform\n",
      "▁Unidos\n",
      "▁union\n",
      "▁units\n",
      "Univers\n",
      "▁municipal\n",
      "▁communication\n",
      "▁Municip\n",
      "▁Universidad\n",
      "▁Universität\n",
      "union\n",
      "▁Unit\n",
      "univers\n",
      "▁opportunity\n",
      "▁statunit\n",
      "▁Junior\n",
      "▁universal\n",
      "▁university\n",
      "▁statunitense\n",
      "▁municipio\n",
      "junit\n",
      "▁estadounidense\n",
      "▁Community\n",
      "▁universitaire\n",
      "▁alcuni\n",
      "▁universe\n",
      "Union\n",
      "▁junior\n",
      "▁uniqu\n",
      "▁Municipal\n",
      "▁municipality\n",
      "▁Universal\n",
      "▁Communic\n",
      "▁communicate\n",
      "▁communities\n",
      "community\n",
      "▁Unicode\n",
      "unix\n",
      "▁Uniti\n",
      "▁UNION\n",
      "▁uniformly\n",
      "▁Unix\n",
      "université\n",
      "▁uninstall\n",
      "unicí\n",
      "communic\n",
      "uning\n",
      "▁unix\n",
      "▁Units\n",
      "▁unicode\n",
      "uniform\n",
      "bin\n",
      "▁bit\n",
      "bit\n",
      "ability\n",
      "▁Bibli\n",
      "big\n",
      "ibility\n",
      "abil\n",
      "▁habit\n",
      "▁big\n",
      "obile\n",
      "bind\n",
      "▁combin\n",
      "▁bij\n",
      "▁bien\n",
      "▁también\n",
      "▁Bill\n",
      "Big\n",
      "▁probability\n",
      "▁Bibliothèque\n",
      "▁bind\n",
      "▁Big\n",
      "▁Bilder\n",
      "bitr\n",
      "▁Bibliografia\n",
      "▁bin\n",
      "Binding\n",
      "▁arbitr\n",
      "▁Biographie\n",
      "▁binding\n",
      "▁bits\n",
      "bild\n",
      "▁combination\n",
      "▁mobile\n",
      "bing\n",
      "bie\n",
      "▁bill\n",
      "▁bird\n",
      "▁arbitrary\n",
      "▁ability\n",
      "▁Bildern\n",
      "abilities\n",
      "▁Bild\n",
      "▁bibli\n",
      "▁birth\n",
      "▁habitants\n",
      "obil\n",
      "▁combined\n",
      "ibil\n",
      "gebiet\n",
      "▁Robin\n",
      "▁bil\n",
      "▁possibility\n",
      "▁bild\n",
      "obierno\n",
      "▁combine\n",
      "▁habitantes\n",
      "bbi\n",
      "ambigu\n",
      "bits\n",
      "Bitmap\n",
      "▁Columbia\n",
      "▁Billboard\n",
      "obi\n",
      "Bind\n",
      "bia\n",
      "▁stabil\n",
      "binom\n",
      "▁bigger\n",
      "mobile\n",
      "▁Bishop\n",
      "▁orbit\n",
      "▁Biography\n",
      "bibli\n",
      "▁Gebiet\n",
      "▁habitat\n",
      "bigg\n",
      "▁Bird\n",
      "▁bio\n",
      "▁Bir\n",
      "▁birds\n",
      "▁Bibliographie\n",
      "ambiguation\n",
      "abile\n",
      "bil\n",
      "▁combinations\n",
      "▁También\n",
      "▁ubic\n",
      "▁Billy\n",
      "disambiguation\n",
      "bib\n",
      "bird\n",
      "▁Bit\n",
      "bió\n",
      "▁Bitte\n",
      "▁Biografia\n",
      "abi\n",
      "bij\n",
      "▁gobierno\n",
      "binding\n",
      "▁inhabitants\n",
      "▁abit\n",
      "▁Bitmap\n",
      "▁Bib\n",
      "▁biz\n",
      "▁Bil\n",
      "bir\n",
      "bildung\n",
      "▁describing\n",
      "▁Bij\n",
      "▁bid\n",
      "▁Colombia\n",
      "▁bitmap\n",
      "▁Bio\n",
      "▁debido\n",
      "Bit\n",
      "▁Mobile\n",
      "▁Bible\n",
      "ibile\n",
      "▁Robinson\n",
      "▁ambigu\n",
      "BIT\n",
      "▁Bischof\n",
      "▁bitter\n",
      "bian\n",
      "▁responsibility\n",
      "Visibility\n",
      "bid\n",
      "▁probabil\n",
      "▁bias\n",
      "bibliothek\n",
      "▁possibilities\n",
      "▁compatibility\n",
      "bio\n",
      "▁biggest\n",
      "▁ambient\n",
      "▁Binding\n",
      "▁stability\n",
      "bindung\n",
      "▁cambio\n",
      "bine\n",
      "▁bishop\n",
      "▁Bien\n",
      "▁gebied\n",
      "▁visibility\n",
      "▁abitanti\n",
      "▁bila\n",
      "▁capabilities\n",
      "▁mobil\n",
      "▁Bin\n",
      "▁possibil\n",
      "bishop\n",
      "visibility\n",
      "▁cabin\n",
      "▁cabinet\n",
      "▁Bibliografía\n",
      "▁exhibition\n",
      "Mobile\n",
      "▁combining\n",
      "▁bisher\n",
      "▁Bind\n",
      "▁bib\n",
      "▁bir\n",
      "▁binnen\n",
      "▁Bian\n",
      "tring\n",
      "String\n",
      "▁string\n",
      "▁String\n",
      "string\n",
      "▁tried\n",
      "▁matrix\n",
      "▁distribution\n",
      "matrix\n",
      "▁attribute\n",
      "▁strings\n",
      "▁district\n",
      "Attribute\n",
      "istrict\n",
      "▁Altri\n",
      "▁trigger\n",
      "▁District\n",
      "toString\n",
      "▁altri\n",
      "ToString\n",
      "▁attributes\n",
      "▁trick\n",
      "▁restrict\n",
      "▁strict\n",
      "▁trib\n",
      "▁entries\n",
      "▁stri\n",
      "▁retrieve\n",
      "▁countries\n",
      "▁contribution\n",
      "pmatrix\n",
      "▁trivial\n",
      "▁electric\n",
      "▁metric\n",
      "attribute\n",
      "▁distributed\n",
      "▁matrices\n",
      "getString\n",
      "ometric\n",
      "▁trial\n",
      "NSString\n",
      "bmatrix\n",
      "▁tries\n",
      "ctrine\n",
      "Matrix\n",
      "▁Patrick\n",
      "trim\n",
      "attributes\n",
      "Attributes\n",
      "▁NSString\n",
      "▁trig\n",
      "metric\n",
      "▁Trib\n",
      "▁patri\n",
      "strip\n",
      "▁trim\n",
      "▁Austria\n",
      "atrice\n",
      "▁trip\n",
      "▁contrib\n",
      "▁Distribution\n",
      "▁strip\n",
      "▁symmetric\n",
      "▁industrial\n",
      "▁strictly\n",
      "▁distributions\n",
      "▁Retrie\n",
      "▁strik\n",
      "strings\n",
      "▁triggered\n",
      "Trigger\n",
      "substring\n",
      "▁distribu\n",
      "▁contributions\n",
      "STRING\n",
      "trigger\n",
      "▁industri\n",
      "▁strike\n",
      "▁distrito\n",
      "▁metrics\n",
      "contrib\n",
      "stringify\n",
      "▁restricted\n",
      "▁Matrix\n",
      "▁doctrine\n",
      "strij\n",
      "▁distrib\n",
      "▁Patri\n",
      "▁triggers\n",
      "▁Attribute\n",
      "▁triumph\n",
      "▁districts\n",
      "▁restriction\n",
      "atri\n",
      "▁restrictions\n",
      "▁Retrieved\n",
      "▁StringBuilder\n",
      "▁geometric\n",
      "▁Betrieb\n",
      "Strings\n",
      "entries\n",
      "▁contributed\n",
      "▁Electric\n",
      "▁retrieved\n",
      "setAttribute\n",
      "distribution\n",
      "▁trif\n",
      "▁contribu\n",
      "▁substring\n",
      "▁tricky\n",
      "trightarrow\n",
      "▁contribute\n",
      "▁striking\n",
      "▁tribes\n",
      "tritt\n",
      "▁attributed\n",
      "▁Tried\n",
      "▁tribe\n",
      "▁squadra\n",
      "▁quadratic\n",
      "▁Squadron\n",
      "▁sexual\n",
      "osex\n",
      "▁doctor\n",
      "▁Doctor\n",
      "DOCTYPE\n",
      "▁doctrine\n",
      "▁doctor\n",
      "▁Doctor\n",
      "▁non\n",
      "non\n",
      "▁Non\n",
      "Non\n",
      "nonumber\n",
      "▁canon\n",
      "anon\n",
      "▁anonymous\n",
      "annon\n",
      "▁canonical\n",
      "anonymous\n",
      "ignon\n",
      "▁decl\n",
      "▁decla\n",
      "▁declar\n",
      "▁declared\n",
      "▁decided\n",
      "▁decre\n",
      "▁declare\n",
      "▁decor\n",
      "▁decision\n",
      "▁decide\n",
      "▁declaration\n",
      "decode\n",
      "deck\n",
      "▁decom\n",
      "▁decid\n",
      "decor\n",
      "▁deck\n",
      "▁decay\n",
      "▁decode\n",
      "▁decis\n",
      "▁dece\n",
      "▁decrease\n",
      "▁declaring\n",
      "▁médec\n",
      "▁decomposition\n",
      "▁DECLARE\n",
      "▁decent\n",
      "▁decir\n",
      "decl\n",
      "▁declarations\n"
     ]
    }
   ],
   "source": [
    "# tokens with a latin prefix related to a digit\n",
    "latin_prefix=['uni', 'bi', 'duo', 'tri', 'quadr', 'quattuor', 'quint', \n",
    "              'quinque', 'sext', 'sex', 'sept', 'septem', 'oct', 'octo',\n",
    "              'non', 'novem', 'dec', 'decem']\n",
    "for word in latin_prefix:\n",
    "    for k,v in tokenizer.get_vocab().items():\n",
    "        if word.lower() in k.lower() and k not in filtered_vocab.keys():\n",
    "            print(k)\n",
    "            #ignored_tokens.append({'token_ignored': k, 'word': word})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e56d0c",
   "metadata": {},
   "source": [
    "## Export CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71edc099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:30:01.817817Z",
     "start_time": "2023-11-09T18:30:01.767874Z"
    }
   },
   "outputs": [],
   "source": [
    "list_filtered_vocab = [{'token_id': v, 'token_str': k} for k,v in filtered_vocab.items()]\n",
    "df = pd.DataFrame(list_filtered_vocab)\n",
    "df.to_csv(f'../data/filter_tokens/filter_token_number_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d0033f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T18:30:01.818173Z",
     "start_time": "2023-11-09T18:30:01.768390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'vicuna'"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Minimal filtering\n",
    "\n",
    "Only filter digits to avoid verbatim output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8666ff704ceb6ece"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 words to filter\n",
      "10 tokens filtered\n"
     ]
    }
   ],
   "source": [
    "df_words = pd.read_csv('../data/filter_tokens/filter_words_number_minimal.csv', dtype=str, header=None)\n",
    "list_words = df_words[0].to_list()\n",
    "print(f'{len(list_words)} words to filter')\n",
    "\n",
    "filtered_vocab = {}\n",
    "for word in list_words:\n",
    "    #print(word)\n",
    "    for k,v in tokenizer.get_vocab().items():\n",
    "        if word.lower() == k.lower().strip('▁').rstrip('s'):\n",
    "            # ignore case, remove space token, remove plural (s)\n",
    "            filtered_vocab[k] = v\n",
    "\n",
    "print(f'{len(filtered_vocab)} tokens filtered')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:37:44.464143Z",
     "start_time": "2023-12-22T07:37:42.440312Z"
    }
   },
   "id": "40b36954da493530"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'0': 29900,\n '1': 29896,\n '2': 29906,\n '3': 29941,\n '4': 29946,\n '5': 29945,\n '6': 29953,\n '7': 29955,\n '8': 29947,\n '9': 29929}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_vocab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:37:57.301675Z",
     "start_time": "2023-12-22T07:37:57.249714Z"
    }
   },
   "id": "957ae0eb2c829f67"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "list_filtered_vocab = [{'token_id': v, 'token_str': k} for k,v in filtered_vocab.items()]\n",
    "df = pd.DataFrame(list_filtered_vocab)\n",
    "df.to_csv(f'../data/filter_tokens/filter_token_number_minimal_{model_name}.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:38:37.990762Z",
     "start_time": "2023-12-22T07:38:37.898241Z"
    }
   },
   "id": "f80b5cd5c0ab36f5"
  },
  {
   "cell_type": "markdown",
   "id": "74b416d8",
   "metadata": {},
   "source": [
    "----------\n",
    "----------\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "17"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens of target string\n",
    "len(tokenizer.encode('Sure, here is a random string of 4 digits: 6532', add_special_tokens=False))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a5bbf3ec5f5c3b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4118e596a4117b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
